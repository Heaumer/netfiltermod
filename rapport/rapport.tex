% pdflatex rapport.tex; pdflatex rapport.tex; pdflatex rapport.tex
\documentclass[a4paper]{article}

\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}

% shorten margin
\usepackage[]{fullpage}

\title{Création d'un framework pour des routeurs
	avec support pour notification de vitesse explicite
	(kernel space)\\Tuteur : Dino LOPEZ}
\author{Mathieu BIVERT, François CHAPUIS, Calypso PETIT, Sophie VALENTIN}

\makeatletter
\def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill \kern \z@}
\def\maketitle{%
  \null
  \thispagestyle{empty}%
  \vskip 1cm
  \begin{center}
        \normalfont\large\@author
  \end{center}
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \hrule height 2pt
  \par
  \begin{center}
        \huge \strut \@title \par
        \@date
  \end{center}
  \hrule height 2pt
  \par
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil  
  \vfil  
  \vfil
  \vfil  
  \vfil
  \vfil
  \vfil
  \vfil  
  \vfil  
  \vfil
  \vfil  
  \vfil  
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \vfil
  \null
\begin{figure}[!ht]
        \centering
        \includegraphics[scale=.5]{polytech.png}
\end{figure}
\vfil
\cleardoublepage
}
\makeatother

\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Introduction}

Tous les réseaux de type Ethernet nécessitent un contrôle de congestion. En effet,
l'augmentation du traffic, et la distribution non-équitable des ressources 
entre les usagers, peuvent conduire dans le meilleur cas à un
ralentissement des communications et dans le pire cas à des \textit{congestion collapses}.
La clé du succès de l'Internet est la mise en place du contrôle de congestion.

Une famille de protocoles de contrôle de congestion est l'ERN
(Explicit Rate Notification) \cite{thesis1}. Ils reposent sur les informations
envoyées/calculées par des routeurs, comme le taux d'émission
optimal. 

Dans le cadre de notre projet, nous devons élaborer un
module noyau à destination d'un routeur supportant l'ERN.
Un tel module établiera des statistiques sur les paquets ERN\footnote{Un paquet ERN est en fait 
un paquet TCP avec des champs additionnels (dans les options TCP). Ces champs additionnels sont : 
le DrdRate (Desired Rate), le RTT (Round-trip delay time) et le cwnd (valeur courante de la fenêtre de congestion)}
qui traversent le routeur. 

Dans un premier temps, nous allons préciser
la problématique du sujet et l'outil principal utilisé. Puis dans un
second temps, nous présenterons les solutions mises en oeuvre pour 
l'élaboration des statistiques et leurs résultats. Enfin, nous concluerons sur ce qui a été fait
et ce qu'il reste à faire.

\section{Description du problème}
\subsection{Contexte}
Un routeur est une machine qui assure l’acheminement des données,
d'un réseau informatique à un autre. Son rôle consiste à diriger
les paquets d'une interface réseau vers une autre par le meilleur chemin\footnote{Le meilleur chemin est défini par
les politiques de routage. Il peut s'agir du chemin le plus court, le plus fiable ou le plus sécurisé.} et selon les règles définies dans la table de
routage. 
Les interfaces réseaux peuvent être assimilées à des
portes par lesquelles les données entrent et sortent du routeur, et
la table de routage à une façon de diriger les paquets à travers
ces portes.
Il s'agit ici de calculer des statistiques nécessaires 
pour le protocole ERN. Un routeur ERN a besoin de connaître :
\begin{itemize}
	\item l'Input Traffic Rate;
	\item le Qsize;
	\item l'Output Capacity Link.
\end{itemize}
Compte tenu de la courte durée du projet et 
du nombre important de concepts à assimiler (programmation de modules,
management de mémoire, mécanismes de concurrence, structure du noyau, ...),
nous nous sommes focalisés sur une statistique particulière : l'occupation
réelle instantanée de la file d'attente, appelée Qsize.

En utilisant l'architecture de Netfilter, nous devons renseigner
un tableau de statistiques comportant, pour chaque interface, le
nombre de paquets entrants à destination de cette interface
(Input Packets, \textit{I}) et le nombre de paquets sortants
(Output Packets, \textit{O}) par cette interface. Le tableau sera de la forme donnée par le tableau \ref{stats}.
\begin{figure}[!ht]
	\centering
	\begin{tabular}{c|c|c|c}
		Nom de l'interface & I & O & Q = I-O \\
		\hline
		eth$0$ & $\ldots$ & $\ldots$ & $\ldots$ \\
		eth$1$ & $\ldots$ & $\ldots$ & $\ldots$ \\
		$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ \\
		eth$n$ & $\ldots$ & $\ldots$ & $\ldots$ \\
	\end{tabular}
	\caption{\label{stats} Statistiques requises par le protocole}
\end{figure}

En comptabilisant I et O,
on pourra déterminer Qsize à tout moment.
Notons que Qsize est associé à une interface de sortie mais tient compte des paquets entrants par toutes les interfaces.

\subsection{Netfilter}
Netfilter est la première piste que nous avons décidé d'explorer pour 
parvenir à calculer Qsize puisque nous avons vu qu'un routeur ERN a été développé en utilisant Netfilter \cite{}.
Netfilter est un framework permettant l'interception des paquets
grâce à des hooks : ce sont des points d'accroche dans le noyau \cite{netfilter1}.
Lorsqu'un événement réseau se produit (entrée d'un paquet par exemple),
une fonction de callback associée à ce type d'évènement est appelée.
A chaque hook, Netfilter permet d'accepter les paquets ou de s'en
débarasser. Pour le protocole IPv4, on compte cinq hooks. Ces derniers
sont organisés comme sur la figure \ref{hooks}.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=.5]{hooks.jpg}
	\caption{\label{hooks} Position des hooks Netfilter}
\end{figure}

Les paquets entrent dans le routeur par le point d'entrée, après avoir 
subi des tests au niveau de la carte réseau. Ils traversent un premier 
hook, appelé \textit{NF\_IP\_PRE\_ROUTING}. Ensuite, après consultation de la table 
de routage, les paquets à destination du routeur sont dirigés vers un 
processus interne et passent par le deuxième hook, \textit{NF\_IP\_LOCAL\_IN}. 
Quant aux autres paquets, ils sont dirigés vers une interface de sortie 
et traversent successivement le troisième hook, \textit{NF\_IP\_FORWARD} puis le 
quatrième, \textit{NF\_IP\_POST\_ROUTING}, avant de sortie du routeur. Les paquets 
peuvent aussi être créés par le routeur lui-même, ils passent alors 
par le hook \textit{NF\_IP\_LOCAL\_OUT}.

Netfilter va nous permettre, grâce aux hooks, d'ajouter des fonctionnalités au routeur 
sans toucher directement au code du noyau Linux. Afin d'utiliser Netfilter, il faut 
programmer un module en mode noyau. Les modules sont des morceaux de code écrits en 
langage C, pouvant être ajoutés ou retirés du noyau dynamiquement \cite{module1}. Ils apportent de 
nouvelles fonctionnalités au noyau. Dans notre cas, le module fournira des fonctions 
dites de callback qui seront automatiquement appelées lorsqu'un paquet traversera le 
hook associé à la fonction. Ces fonctions serviront à calculer les statistiques que 
nous devons fournir durant ce projet.
En ce qui concerne la programmation des modules en mode noyau sous Linux, elle est 
vraiment différente de celle en mode utilisateur. En effet, les bibliothèques sont 
plus restreintes et le débogage plus ardu, car le moindre accès mémoire invalide peut 
rendre le système inutilisable. Les détails techniques concernant ce type de programmation 
sont évoqués en annexe [numéro de l'annexe].

\section{Environnement de tests}

\subsection{Architecture}

Comme nous l'avons dit ci-dessus, la programmation en mode noyau peut avoir un grave 
impact sur le fonctionnement de l'ordinateur en cas de bug. De ce fait, nous avons 
décidé d'utiliser des machines virtuelles. Celles-ci vont également nous permettre 
de mettre aisément en place les interfaces réseaux nécessaires aux tests.La mise en 
place des machines virtuelles se fera grâce au logiciel VirtualBox\cite{virtbox}.

Dans un premier temps, on souhaite mettre en place une
architecture réseau simple, constituée de trois machines, la
figure \ref{topo} indiquant la topologie réseau suivie:
\begin{itemize}
	\item un émetteur;
	\item un routeur;
	\item et un récépteur.
\end{itemize}

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=.5]{topo.jpg}
	\caption{\label{topo} Topologie du réseau}
\end{figure}

Pour cela nous utilisons trois machines virtuelles lancées sur la même machine hôte. 
Comme un routeur sert à faire transiter des données d'un réseau à un autre, deux
réseaux sont nécessaires ici : un sur lequel se trouve l'émetteur et un pour le récepteur.
VirtualBox permet de créer des réseaux locaux et les différentes machines virtuelles
 peuvent appartenir au réseau local que l’on souhaite \cite{virtualbx}. Nous créons donc deux réseaux locaux 
 comme dans la figure \ref{reseaux1}.

\begin{figure}[!ht]
	\centering
	\begin{tabular}{c|c|c}
	Nom du réseau & CIDR & Masque de sous-réseau\\
	lan\_emetteur & 100.100.100.0/24 & 255.255.255.0\\
	lan\_recepteur & 200.200.200.0/24 & 255.255.255.0\\
	\end{tabular}
	\caption{\label{reseaux1} Les deux réseaux}
\end{figure}

Sur chaque machine virtuelle, il faut configurer les interfaces réseaux et la table de routage. 
Ici l'émetteur peut atteindre le routeur. De même, le récepteur peut atteindre le routeur. 
En revanche, le routeur sert de passerelle entre l'émetteur et le récepteur.
Le routeur doit simplement être connecté aux deux réseaux.
Cela est possible car il dispose de plusieurs interfaces réseaux appelées eth0, eth1, eth2...
Chacune des deux interfaces du routeur doit donc être
connectée au bon réseau:
\begin{verbatim}
  # ifconfig eth1 100.100.100.2 netmask 255.255.255.0
  # ifconfig eth2 200.200.200.2 netmask 255.255.255.0
\end{verbatim}

Enfin, il ne faut pas oublier d'activer le transfert de
paquets :
\begin{verbatim}
  # echo 1 > /proc/sys/net/ipv4/ip_forward
\end{verbatim}

Du côté de l'émetteur, l'interface eth$0$ est reliée à
\textit{lan\_emetteur}, et il faut passer par 100.100.100.2
(c'est-à-dire l'interface eth$1$ du routeur) pour transmettre
des paquets à destination du réseau lan\_recepteur:
\begin{verbatim}
  # ifconfig eth0 100.100.100.1 netmask 255.255.255.0
  # route add -net 200.200.200.0 netmask 255.255.255.0 gw 100.100.100.2 eth0
\end{verbatim}

Côté récepteur, la configuration est analogue à celle de l'émetteur:
\begin{verbatim}
  # ifconfig eth0 200.200.200.1 netmask 255.255.255.0
  # route add -net 100.100.100.0 netmask 255.255.255.0 gw 200.200.200.2 eth0
\end{verbatim}

\subsection{Test de connectivité}
En guise de vérification, on essaye de \textit{pinger} le
récépteur depuis l'émetteur (fig.\ref{ping1}):

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=.5]{ping.jpg}
	\caption{\label{ping1} Émetteur et récépteur peuvent communiquer}
\end{figure}

La commande \textit{ping} n'est pas suffisante pour envoyer
des paquets dans le cadre de notre projet car elle utilise
le protocole ICMP et non le protocole IP. Par conséquent,
nous avons cherché un outil utilisant les protocoles IP et TCP.
La commande \textit{iperf} permet d'envoyer entre un client et
un serveur des paquets TCP ou UDP. Nous l'avons donc utilisé
entre l'émetteur et le récépteur. Par défaut \textit{iperf}
utilise le protocole TCP.

Côté récépteur, on lance \textit{iperf} en mode serveur:
\begin{verbatim}
  # iperf -s
\end{verbatim}

et côté émetteur, on envoie des paquets sur le récépteur:
\begin{verbatim}
  # iperf -c 200.200.200.1
\end{verbatim}

L'option \textit{-i} d'\textit{iperf} peut-être utilisée
pour fixer l'intervalle de temps entre les rapports
d'envois de paquets (mesure bande passante, $\ldots$).

\section{Calcul de la taille de la file d'attente avec Netfilter}
A présent, nous écrivons un module utilisant Netfilter. Les
fonctions de callback données par les hooks permettent
d'accéder à un \textit{struct sk\_buff} \cite{skbuff}. Cette structure représente
un paquet dans le noyau Linux. Il est ainsi possible de récupérer
les informations du datagramme telles que le protocole ou encore
l'adresse IP de destination.

Les fonctions de callback fournissent également deux structures
de type \textit{struct net\_device} : une pour l'interface réseau
d'entrée et une autre pour l'interface réseau de sortie. Avec
une telle structure, on peut connaître le nom de l'interface
grâce au champ \textit{net\_device.name}.

Cependant, ces structures ne sont pas toujours remplies par le noyau : cela 
dépend du hook dans lequel nous nous trouvons. En effet, dans le hook \textit{NF\_IP\_PRE\_ROUTING}, 
seule l'interface réseau d'entrée du paquet est connue. Dans le hook \textit{NF\_IP\_POST\_ROUTING}, 
c'est le contraire : seule l'interface de sortie est renseignée. En revanche, le hook \textit{NF\_IP\_FORWARD} 
connait les deux interfaces.
Pour déterminer où se trouve la file d'attente, nous devons travailler avec deux hooks : 
le premier hook doit se trouver en amont de l'emplacement supposé de la file d'attente et le second en aval. 
Dans le premier hook, l'input trafic rate de l'interface de sortie est incrémenté et, dans le second hook, 
l'output trafic rate de l'interface de sortie est incrémenté.
 
Après chaque opération, on met à jour la taille de la file
d'attente de l'interface de sortie en soustrayant l'Input Traffic
Rate par l'Output Traffic Rate.

Les deux hooks pouvant accéder aux mêmes données en même temps,
il est indispensable de mettre en place un mécanisme de
synchronisation. Les hooks étant déclenchés par des interruptions,
il est impossible d'utiliser des mutex ou encore des sémaphores \cite{mutex}.
Le noyau fournit des spinlocks, jouant le même rôle, mais dans
un contexte d'interruptions. \cite{lock}

Les hooks concernant les paquets qui traversent le routeur sont :
\textit{NF\_IP\_PRE\_ROUTING}, \textit{NF\_IP\_FORWARD} et
\textit{NF\_IP\_POST\_ROUTING}. Parmi ces hooks, seuls
\textit{NF\_IP\_FORWARD} et \textit{NF\_IP\_POST\_ROUTING}
contiennent l'information sur l'interface de sortie dans la
structure \textit{net\_device}. Par conséquent, nous allons
tout d'abords chercher la taille de la pile entre
\textit{NF\_IP\_FORWARD} et \textit{NF\_IP\_POST\_ROUTING}.

\subsection{\textit{NF\_IP\_FORWARD} et \textit{NF\_IP\_POST\_ROUTING}}
Pour commencer, les vitesses maximales des interfaces eth$1$ et
eth$2$ sont configurées de la même façon. Ainsi, on s'attend à
ce que la sortie des paquets soit fluide : nous ne nous attendons
pas à la création d'une file d'attente interne.L'exécution du module
utilisant ces deux hooks donne l'extrait de trace en
fig.\ref{forwardpost}.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=.5]{forward_post.jpg}
	\caption{\label{forwardpost} Trace obtenue entre FORWARD et POST}
\end{figure}

Après examination entière de la trace, on constate que Qsize
pour eth$1$ et Qsize pour eth$2$ sont constamment égaux à zéro.
On peut tout de même vérifier que le nombre de paquets entrants
est incrémenté correctement à chaque exécution de la fonction
de callback de \textit{NF\_IP\_FORWARD}. Quant au nombre de paquets
sortants, il est incrémenté correctement à chaque exécution de la
fonction de callback du hook \textit{NF\_IP\_POST\_ROUTING}.

À présent, on configure la bande passante maximale de l'interface
eth$2$ afin que la vitesse maximale soit de $10$ Mbits/seconde.
Pour cela, on utilise la commande \textit{tc} (traffic control) \cite{tc1} \cite{tcman}.
Cette commande va agir sur le fonctionnement des files d’attente
en sortie d’une interface donnée.
\begin{verbatim}
  # tc qdisc add dev eth2 root handle 1: tbf rate 10mbit buffer 1600 limit 3000
  # tc qdisc add dev eth2 parent 1: handle 10: netem delay 3ms
\end{verbatim}
Pour limiter la bande passante, on utilise un Token Bucket Filter
(tbf) en précisant qu’on ne peut pas dépasser un débit de
$10$Mbits/seconde.

Après avoir ainsi configuré la vitesse maximale,
nous nous attendons à obtenir une file d’attente non nulle.
Les résultats des tests effectués nous montrent que la taille
de la file d’attente est toujours nulle, ce qui prouve que la
file d’attente interne ne se trouve pas entre les hooks
\textit{NF\_IP\_FORWARD} et \textit{NF\_IP\_POST\_ROUTING}.

Il nous faut donc regarder entre deux autres 
hooks : \textit{NF\_IP\_PRE\_ROUTING} et \textit{NF\_IP\_POST\_ROUTING}.

\subsection{\textit{NF\_IP\_PRE\_ROUTING} et \textit{NF\_IP\_POST\_ROUTING}}
L'interface de sortie n'étant pas disponible dans le hook \textit{NF\_IP\_PRE\_ROUTING},
il va nous falloir effectuer le routage à la main pour déterminer à quelle interface de sortie le paquet est destiné. Notre
objectif étant de chercher où se trouve la file, nous nous
contentons d'un routage manuel, avec des adresse IP fixes dans le code.

Nous obtenons finalement le même résultats que précédemment. 
Il semble donc qu'il soit impossible d'utiliser Netfilter pour
obtenir les statistiques. 

\subsection{Lecture des statistiques sur les cartes réseaux}
Grâce à une fonction du noyau, il est possible d’accéder à des
statistiques concernant les interfaces. La fonction \textit{dev\_get\_stats}
définie dans \textit{linux/netdevice.h} fournit :
\begin{itemize}
	\item le nombre de paquets (ou octets) reçus sur une interface;
	\item le nombre de paquets (ou octets) transmis par une interface;
	\item le nombre d’erreurs;
	\item et le nombre de paquets “dropped”.
\end{itemize}

Ces statistiques pourraient nous aider à calculer Qsize. Cependant,
elles ne permettent pas de différencier les paquets TCP des autres paquets.
Il nous faut donc aller un peu plus loin dans le code.

\section{Localisation des files d'attente dans la couche liaison}

Après tous les tests que nous avons effectué précédemment, nous pouvons affirmer
 que la file d'attente contenant les paquets qui n'ont pas encore été traités ne
  se situe pas au niveau du routeur, c'est-à-dire dans la couche Réseau du modèle 
  OSI. Par conséquent, nous allons tenter de la localiser dans la couche
   inférieure du modèle OSI : la couche Liaison. Voici le schéma représentant
    les différentes phases du traitement d'un paquet :

Schéma !

\subsection{Les files d'attente avant le routage}
Comme on peut le voir sur ce schéma issu de nos recherches, à la reception des paquets, ces derniers sont stockés dans une file d'attente appelée backlog \cite{10poin} ou ingress queue, dans la couche Liaison. Il en existe une par processeur. 
Lorsqu'un paquet entre, une interruption appelée \textit{NET\_RX\_SOFTIRQ} est levée. On peut choisir de traiter cette interruption grâce à un handler qui est une fonction appelée automatiquement lorsque l'interruption est levée. Nous avons donc essayer d'incrémenter le nombre de paquets dans ce handler et de continuer à le décrémenter  dans le hook \textit{NF\_IP\_POST\_ROUTING}. Malheureusement, le traitement des interruptions en mode noyau est avéré être plus compliqué que ce que nous pensions. Et nous n'avons pas voulu perdre de temps sur ce point puisque, par ailleurs, les recherches que nous avons fait nous ont appris que ces files d'attente sont liées à la vitesse de traitement des paquets \cite{journey} \cite{bookin}. De telles files ne nous intéressent donc pas car nous nous intéressons uniquement aux files d'attente liées aux vitesses d'émission des paquets. 

\subsection{Les files d'attente après le routage}

\subsubsection{File d'attente entre les fonctions dev\_queue\_xmit et hard\_start\_xmit}

Après le passage dans les hooks Netfilter, le paquet traverse la couche Liaison \cite{intel} \cite{stack}. A ce niveau, il y a une mise en file d’émission pour chaque interface de sortie. Cette file d’émission existe entre la fonction noyau \textit{dev\_queue\_xmit} et la fonction \textit{hard\_start\_xmit}. La première fonction empile le paquet lorsqu’il passe de la couche Réseau à la couche Liaison. La deuxième fonction a pour but d’indiquer au noyau que les paquets ont été transmis à la carte réseau. Nous pouvons donc incrémenter le nombre de paquets au niveau de la fonction \textit{dev\_queue\_xmit}, qui prend en compte l’interface de sortie.

Il n’est pas possible d’écrire un code modulaire pour ce type de manipulation. Nous sommes contraints de modifier le code du noyau. Cela implique la compilation d’un nouveau noyau, tâche assez lourde. 

\subsubsection{La compilation du noyau Linux}

Pour pouvoir compiler un noyau Linux \cite{compil}, il nous a fallu télécharger son code source. 
Tout d'abord, nous avons télécharger sur le web une des versions les plus récentes. 
Mais, avec cette version, nous n'avions pas le bon fichier de configuration. De ce fait, 
la compilation du noyau prenait énormément d'espace sur le disque dur de la machine virtuelle 
et cela a rendu inutilisable un certain nombre de machines virtuelles malgré nos tentatives 
de leur allouer d'avantage d'espace disque. Cette étape nous a fait perdre beaucoup de temps.
Finalement, nous avons téléchargé un autre noyau moins récent grâce à la commande :
\begin{verbatim}
apt_get install linux-source-2.6.32
\end{verbatim}
Nous avons choisi la version 2.6.32 car c'est celle que nous avions sur nos machines, 
cela nous a permis de récupérer le fichier de configuration et la compilation prend beaucoup moins de place.
Après avoir téléchargé le noyau, il y a plusieurs étapes pour le compiler. 
Le téléchargement nous procure un fichier compressé, il faut donc le décompresser :
\begin{verbatim}
# tar xvjf linux-2.6.32.tar.bz2
\end{verbatim}
Une fois la décompression effectuée, il faut placer le fichier de configuration trouvé dans /boot dans le dossier contenant le noyau et le renommer en .config.
Ensuite, il faut créer l'image binaire du noyau qui servira pour booter sur notre nouveau noyau :
\begin{verbatim}
# make bzImage
\end{verbatim}
Les modules, qui comprennent notamment les pilotes utiles pour l'utilisation du clavier et de la souris par exemple, peuvent être compilés et installés :
\begin{verbatim}
# make modules
# make modules_install
\end{verbatim}
Enfin, on peut compiler et installer le noyau lui-même :
\begin{verbatim}
# make
# make install
\end{verbatim}
Pour pouvoir booter sur ce noyau ainsi installé, il faut taper la commande :
\begin{verbatim}
# update-grub
\end{verbatim}
et redémarrer l'ordinateur.

A chaque modification d'un des fichiers sources du noyau, il faut refaire le make et le make install.

\subsubsection{Un premier essai dans dev\_queue\_xmit}
En sortie, il existe une politique de gestion de files d'attente appelée qdisc. Il s'agit d'une structure 
comportant deux pointeurs sur fonction enqueue et dequeue. Il existe une qdisc par interface de sortie.
Nous avons donc cherché à connaître le contenu effectif en paquets TCP de la file d'attente.
Il est clair que l'incrément du nombre de paquets TCP doit se faire au début du corps de la fonction \textit{dev\_queue\_xmit}
quand un appel à la fonction pointée par enqueue est fait. En revanche, nous avons vu que la fonction 
\textit{hard\_start\_xmit} est implémentée au niveau des drivers. Cela implique que l'implémentation de la fonction 
diffère selon la carte réseau. Pour pallier ce problème, nous voulons décrémenter le nombre de paquets à un 
niveau supérieur : par exemple au niveau de l'appel à la fonction pointée par dequeue.
Cependant, le paquet peut très bien êtredépilé de \textit{qdisc}, mais ne pas
être envoyé. Il est donc impératif de se placer à un niveau inférieur.

\subsubsection{Wrapper pour les drivers ethernet (npkt--)}
Les fonctions permettant de manipuler une carte ethernet sont sous
linux abstraite par une structure : ajouter un driver revient donc
à déclarer une structure et à remplir les différents champs, avec
les fonctions adaptées à la carte.
La structure \textit{net\_device\_ops}, déclarée dans
\textit{linux/netdevice.h} joue ce rôle:
\begin{verbatim}
struct net_device_ops {
    int (*ndo_init)(struct net_device *dev);
    void (*ndo_uninit)(struct net_device *dev);
    int (*ndo_open)(struct net_device *dev);
    int (*ndo_stop)(struct net_device *dev);
    netdev_tx_t (*ndo_start_xmit) (struct sk_buff *skb, struct net_device *dev);
    ...
};
\end{verbatim}

Et un driver, par exemple e1000, l'utilise ainsi:
\begin{verbatim}
static const struct net_device_ops e1000_netdev_ops = {
    .ndo_open               = e1000_open,
    .ndo_stop               = e1000_close,
    .ndo_start_xmit         = e1000_xmit_frame,
    .ndo_get_stats          = e1000_get_stats,
    ...
};
\end{verbatim}

La fonction \textit{dev\_hard\_start\_xmit}, située dans
\textit{net/core/dev.c}, est chargée de transmettre le paquet
à la carte. Après quelques tests à l'aide de \textit{printk} bien
placé, nous avons constaté que dans tous les cas, la fonction
est appellée, plus ou moins directement, et qu'elle contient 
un appel à \textit{ndo\_start\_xmit} qui lancera la transmission réelle des paquets. La décrementation de la
taille de la pile peut donc être effectuée ici, plutôt que de
l'être dans le noyau.

\section{Résultats}
En réglant la vitesse et la taille du buffer avec tc, on peut observer
une augmentation de qsize : les paquets rentrent, mais ne sortent jamais.
L'hypothèse que nous avons par la suite vérifiée est que ces paquets
sont droppés par la carte : en regardant la valeur de retour (\textit{rc}) à
la  fin de \textit{dev\_queue\_xmit}, on voit bien  qu'elle est à
\textit{NET\_XMIT\_DROP} à chaque fois que qsize n'est pas décrémenté
en sortie:
\begin{figure}[!ht]
	\centering
	\includegraphics[scale=.5]{dropped.png}
	\caption{\label{dropped} Après réglage avec tc(1)}
\end{figure}

Sans effectuer de  réglages, les paquets ne sont pas droppés, et, selon
des facteurs indeterminés (vitesse carte? TODO), nous observons soit un
qsize toujours à 0, soit un qsize grandissant à l'occasion, mais se
réduisant par la suite:
\begin{figure}[!ht]
	\centering
	\includegraphics[scale=.5]{qsize_default.png}
	\caption{\label{qsize_default} Sans réglage}
\end{figure}


\newpage
\bibliographystyle{ieeetr}
\bibliography{docs}

\textbf{Autre documentation consultée:}
\begin{itemize}
	\item Commande mii-tool (configuration vitesse) : 
	http://www.cyberciti.biz/faq/linux-change-the-speed-and-duplex-settings-of-an-ethernet-card/
	\item Source du noyau : http://lxr.linux.no/
	\item COMPLETER
\end{itemize}


\newpage
\appendix

\section{Compléments sur la programmation de modules Linux}
\subsection{Organisation générale}
\subsection{Mise en place d'un hook netfilter}
\subsection{Synchronisation (spinlock)}
\subsection{Compilation d'un noyau Linux}

\end{document}
